---
title: "Homework 2 Question 1"
author: "Rick Gray"
date: '2022-06-12'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(astsa)
library(readr)
```

### Part A

```{r}
x <- ts(read_csv("ARMAData1.csv", show_col_types = F)[2] %>% pull(x))
z <- c(1, 2, 3)
combs <- crossing(p = z, q = z)

fit_models <- function(row) {
  if (row$p != 2 || row$q != 2) {
    print(paste("p:", row$p, "q:", row$q, " AICc:", sarima(x, row$p, 0, row$q, no.constant = T, details = F)$AICc))
  }
}

walk(1:nrow(combs), ~fit_models(combs[., ]))
```

The optimal model seen below was found using the code above:

<center>$ARMA(3, 1)$</center>

with the following parameters:

<center>$p=3, q=1$</center>

and the resulting AICc:

<center>$AICc=2.964$</center>

### Part B

```{r}
fit <- sarima(x, 3, 0, 1, no.constant = TRUE, details = FALSE)

fit$fit$coef
fit$fit$sigma2
```

The ARMA(3, 1) model for this dataset is as follows:

<center>$x_{t} = w_{t} + 0.7996x_{t-1} + 0.5480x_{t-2} - 0.7759x_{t-3} + 0.5535w_{t-1}$</center>

where

<center>$\sigma_{w}^{2} = 1.0284$</center>

### Part C

```{r}
x <- ts(read_csv("ARMAData2.csv", show_col_types = FALSE)[2] %>% pull(x))
x_train <- window(x, start = 1, end = 150)

fit_train <- sarima(x_train, 2, 0, 2, no.constant = TRUE, details = FALSE)
fit_train$AICc
```

<center>$ARMA(2, 2)$</center>

The resulting AICc for the model specified above is

<center>$AICc=3.034$</center>

### Part D

```{r fig.width=10}
fit_for <- sarima.for(x_train, 100, 2, 0, 2, plot = F)

n <- 250
n_train <- 150

fit_data <- bind_rows(
  data.frame(Time = 1:n,
             Type = factor(rep("Simulated Data", n),
                              levels = c("Simulated Data",
                                         "Pred")),
             x = as.numeric(x)),
  data.frame(Time = 1:n,
             Type = factor(rep("Pred", n),
                              levels = c("Simulated Data",
                                         "Pred")),
             x = c(as.numeric(x_train) - 
                     as.numeric(resid(fit_train$fit)), 
                   as.numeric(fit_for$pred))))
fit_pred_data <- data.frame(Time = 1:n,
                            x = c(as.numeric(x_train) - 
                                    as.numeric(resid(fit_train$fit)), 
                                  as.numeric(fit_for$pred)),
                            SE = c(rep(sqrt(fit_train$fit$sigma2), n_train), 
                                   as.numeric(fit_for$se)))

gg_fit <- ggplot(fit_data,
                 aes(x = Time)) +
  geom_line(aes(y = x, col = Type)) +
  geom_ribbon(data = fit_pred_data,
              aes(x = Time, 
                  ymin = x - 1.96*SE,
                  ymax = x + 1.96*SE),
              alpha = .2) +
  geom_vline(xintercept = n_train) +
  labs(
    title = "95% Confidence Prediction on Time Series Data"
  )

gg_fit
```

Over the training set, the model makes an excellent predictor; it is very close to the true data line. After the training set when making predictions, it initially starts close. It follows the positive trend initially, and drops low on the first dip in the true data. The farther from the start of the predictions loses amplitude, but the cyclical nature of the data continues to be tracked. It can be seen at each hill and valley of the prediction that it matches the peaks and troughs of the true data. After the prediction line flattens, it is no longer useful as a predictor.